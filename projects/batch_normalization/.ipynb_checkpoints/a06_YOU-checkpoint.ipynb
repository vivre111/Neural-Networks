{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A06 Q2: Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a PyTorch wrapper for our UClasses dataset\n",
    "class UClasses(torch.utils.data.Dataset):\n",
    "    def __init__(self, n=300):\n",
    "        super().__init__()\n",
    "        np_ds = utils.UClasses(n=n, binary=False)  # heavy lifting done by NumPy code\n",
    "        self.x = torch.tensor(np_ds.inputs(), dtype=torch.float32)\n",
    "        self.t = torch.argmax(torch.tensor(np_ds.targets()), axis=1, keepdim=False)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.t[idx]\n",
    "    \n",
    "    def inputs(self):\n",
    "        return self.x\n",
    "    \n",
    "    def targets(self):\n",
    "        return self.t\n",
    "            \n",
    "    def plot(self, labels=None, *args, **kwargs): \n",
    "        if labels is None:\n",
    "            labels = self.t\n",
    "        colour_options = ['y', 'r', 'g', 'b', 'k']\n",
    "        if len(labels.size())>1 and len(labels[0])>1:\n",
    "            # one-hot labels\n",
    "            cidx = torch.argmax(labels, axis=1)\n",
    "        else:\n",
    "            # binary labels\n",
    "            cidx = (labels>0.5).type(torch.int)\n",
    "        colours = [colour_options[k] for k in cidx]\n",
    "        plt.scatter(self.x[:,0].detach(), self.x[:,1].detach(), color=colours, marker='.')\n",
    "        plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = UClasses(n=1000)\n",
    "train.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.targets()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. `BatchNorm` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    '''\n",
    "     lyr = BatchNorm(eps=0.001)\n",
    "     \n",
    "     Creates a PyTorch layer (Module) that implements batch normalization, so\n",
    "     that its outputs are remapped. For each node in the layer, the output for the\n",
    "     batch is normalized so that it is zero-mean and approximately unit-variance.\n",
    "     \n",
    "     Inputs:\n",
    "      eps     stability parameter, to avoid division by zero\n",
    "             \n",
    "     Usage:\n",
    "      lyr = BatchNorm(eps=0.001)\n",
    "      y = lyr(x)    # x is a batch (tensor), with one sample in each row\n",
    "                    # y is the same shape as x\n",
    "    '''\n",
    "    def __init__(self, eps=0.001):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x   # replace this line\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Demonstrate `BatchNorm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network base class, `NNBase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network base class saves us from having to duplicate the learn function.\n",
    "\n",
    "class NNBase(nn.Module):\n",
    "    '''\n",
    "     You should not instantiate this class directly. \n",
    "     Base class for other simple neural network classes.\n",
    "     eg.\n",
    "       class MyDerivedNN(NNBase):\n",
    "          ...\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "        self.loss_fcn = None  # Should be overridden in derived class\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "    def learn(self, dl, epochs=10, lr=0.1, plot=True):\n",
    "        '''\n",
    "         net.learn(dl, epochs=10, lr=0.1, plot=True)\n",
    "         \n",
    "         Performs SGD on the neural network.\n",
    "         Inputs:\n",
    "          dl      DataLoader object (PyTorch)\n",
    "          epochs  number of epochs to perform\n",
    "          lr      learning rate\n",
    "          plot    whether or not to plot the learning curve\n",
    "        '''\n",
    "        optim = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            total_loss = 0.\n",
    "            for x,t in dl:\n",
    "                y = self(x)\n",
    "                loss = self.loss_fcn(y, t.float())\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                total_loss += loss.item()\n",
    "            self.losses.append(total_loss/len(dl))\n",
    "        if plot:\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.plot(self.losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Compare learning performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create two network classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NormalNet\n",
    "# Create a simple neural network that does NOT use batchnorm.\n",
    "# You should use NNBase as the base class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNNet\n",
    "# Create a simple neural network that DOES use batchnorm after each\n",
    "# layer (except the output layer).\n",
    "# You should use NNBase as the base class.\n",
    "# Apply batchnorm between the activation function and the connections\n",
    "# to the next layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Let's compare the learning curves for the following cases:\n",
    "1. Normal NN (no batchnorm)\n",
    "3. Batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot the results of the trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
