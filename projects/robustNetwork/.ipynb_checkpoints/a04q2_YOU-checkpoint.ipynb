{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A04-Q2: Combatting Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DividedPlane(torch.utils.data.Dataset):\n",
    "    def __init__(self, n=100, noise=0.1, seed=None):\n",
    "        a = torch.tensor([-0.4, 0.5, 0.15]) #torch.rand((3,))\n",
    "        def myfunc(x):\n",
    "            y = a[0]*x[:,0] + a[1]*x[:,1] + a[2]\n",
    "            return y\n",
    "        self.x = torch.rand((n,2))*2. - 1.\n",
    "        y = myfunc(self.x) + noise*torch.normal( torch.zeros((len(self.x))) )\n",
    "        self.y = (y>0.).type(torch.float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def inputs(self):\n",
    "        return self.x\n",
    "    \n",
    "    def targets(self):\n",
    "        return self.y.reshape( (len(self.y),1) )\n",
    "    \n",
    "    def plot(self, labels=None, *args, **kwargs): \n",
    "        X = self.inputs()\n",
    "        if labels is None:\n",
    "            labels = self.targets()\n",
    "        colour_options = ['y', 'r', 'g', 'b', 'k']\n",
    "        if len(labels[0])>1:\n",
    "            # one-hot labels\n",
    "            cidx = torch.argmax(labels, axis=1)\n",
    "        else:\n",
    "            # binary labels\n",
    "            cidx = (labels>0.5).type(torch.int)\n",
    "        colours = [colour_options[k] for k in cidx]\n",
    "        plt.scatter(X[:,0].detach(), X[:,1].detach(), color=colours, marker='.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DividedPlane(n=100, noise=0.2, seed=165)\n",
    "test = DividedPlane(n=5000)\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(1,2,1); train.plot(); plt.title(f'Training Set');\n",
    "plt.subplot(1,2,2); test.plot(); plt.title(f'Test Set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: `Dropout` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(torch.nn.Module):\n",
    "    '''\n",
    "     lyr = Dropout()\n",
    "     \n",
    "     Creates a dropout layer in which each node is set to zero\n",
    "     with probability lyr.dropprob.\n",
    "     \n",
    "     Usage:\n",
    "       lyr = Dropout()\n",
    "       lyr.set_dropprob(p) # set the dropout probability to p\n",
    "       y = lyr(z)          # sets each node to 0 with probability p\n",
    "       \n",
    "     The input, z, contains one sample per row. So the dropout is\n",
    "     performed independently on each row of z.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropprob = 0.\n",
    "        \n",
    "    def set_dropprob(self, p):\n",
    "        self.dropprob = p\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Drop nodes with prob dropprob\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        y = z  # replace this line\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Dropout layer\n",
    "z = torch.ones((3,1000))\n",
    "drop_layer = Dropout()\n",
    "drop_layer.set_dropprob(0.75)\n",
    "y = drop_layer(z)\n",
    "drop_fraction = (torch.sum(y==0.)*100.)/torch.numel(y)\n",
    "print(f'Dropped {drop_fraction:.1f}%')\n",
    "print(f'Expected output is {torch.sum(y)}, which should be close to {torch.sum(z)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B: `RobustNetwork`\n",
    "* Implement regularization by weight decay <br>\n",
    "* Integrate the dropout layer into learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustNetwork(torch.nn.Module):\n",
    "    def __init__(self, nodes=100):\n",
    "        super().__init__()\n",
    "        self.lyrs = torch.nn.ModuleList()\n",
    "        self.lyrs.append(torch.nn.Linear(2, nodes))\n",
    "        self.lyrs.append(torch.nn.ReLU())\n",
    "        self.lyrs.append(torch.nn.Linear(nodes, nodes))\n",
    "        self.lyrs.append(torch.nn.Sigmoid())\n",
    "        self.drop_lyr = Dropout()    # <-- Create Dropout layer\n",
    "        self.lyrs.append(self.drop_lyr)  # Add it to the list\n",
    "        self.lyrs.append(torch.nn.Linear(nodes, 1))\n",
    "        self.lyrs.append(torch.nn.Sigmoid())\n",
    "        self.loss_fcn = torch.nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for lyr in self.lyrs:\n",
    "            y = lyr(y)\n",
    "        return y\n",
    "\n",
    "    \n",
    "    def learn(self, x, t, epochs=100, lr=0.1, weight_decay=0., dropprob=0.):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            y = self(x)\n",
    "            \n",
    "            loss = self.loss_fcn(y.squeeze(), t.squeeze())\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            self.zero_grad()\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in self.parameters():\n",
    "                    p -= lr*p.grad\n",
    "        plt.plot(np.array(losses))\n",
    "        plt.yscale('log'); plt.xlabel('Epochs'); plt.ylabel('Log Loss');\n",
    "        print(f'Final loss = {loss}')\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_orig = RobustNetwork(nodes=250)\n",
    "\n",
    "# Duplicate the network for apples-to-apples comparison\n",
    "net = copy.deepcopy(net_orig)\n",
    "rnet = copy.deepcopy(net_orig)\n",
    "dnet = copy.deepcopy(net_orig)\n",
    "\n",
    "# Set come common parameters\n",
    "lr = 0.5\n",
    "n_epochs = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading models\n",
    "You might find it helpful to save and load your networks. The lines below save the network, including the connection weights and biases.\n",
    "\n",
    "Note that the pertinent classes have to be declared before you can load an object of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, 'simple_net.pt')\n",
    "#net = torch.load('simple_net.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# No effort to guard against overfitting\n",
    "losses = net.learn(train.inputs(), train.targets(), epochs=n_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# L2 regularization\n",
    "rlosses = rnet.learn(train.inputs(), train.targets(), epochs=n_epochs, lr=lr, weight_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "dlosses = dnet.learn(train.inputs(), train.targets(), epochs=n_epochs, lr=lr, dropprob=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the models\n",
    "#### Let's see what the decision boundaries look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.inputs().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test loss\n",
    "y = net(test.inputs()); test_loss = net.loss_fcn(y, test.targets())\n",
    "ry = rnet(test.inputs()); rtest_loss = rnet.loss_fcn(ry, test.targets())\n",
    "dy = dnet(test.inputs()); dtest_loss = dnet.loss_fcn(dy, test.targets())\n",
    "\n",
    "# Display the results\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,3,1)\n",
    "test.plot(labels=net(test.inputs())); plt.title(f'Orig Test Loss = {test_loss:.3f}')\n",
    "plt.subplot(1,3,2)\n",
    "test.plot(labels=rnet(test.inputs())); plt.title(f'Weight Decay Test Loss = {rtest_loss:.3f}')\n",
    "plt.subplot(1,3,3)\n",
    "test.plot(labels=dnet(test.inputs())); plt.title(f'Dropout Test Loss = {dtest_loss:.3f}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
